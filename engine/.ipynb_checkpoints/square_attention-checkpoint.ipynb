{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepFillv1 experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # will suppress all warnings\n",
    "import neuralgym as ng\n",
    "\n",
    "from deepfillv1.inpaint_model import InpaintCAModel\n",
    "\n",
    "from utils import Namespace\n",
    "from utils import get_colormap_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h = 170, 128\n",
    "w_resize, h_resize = w * 4, h * 4\n",
    "\n",
    "sess_config = tf.ConfigProto()\n",
    "sess_config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "    colormap = get_colormap_image(w, h, w_resize, h_resize)\n",
    "    colormap = (colormap + 1.) * 127.5\n",
    "    colormap = tf.reverse(colormap, [-1])\n",
    "    colormap = tf.saturate_cast(colormap, tf.uint8)\n",
    "    colormap = sess.run(colormap)\n",
    "    colormap = np.array(colormap)\n",
    "    cv2.imwrite('colorpalette.jpg', colormap[0][:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0          -     -     -     -     -     -   -              \n",
      "    1          -     -     -     -     -     -   -              \n",
      "    2          -     -     -     -     -     -   -              \n",
      "    3          -     -     -     -     -     -   -              \n",
      "    4          -     -     -     -     -     -   -              \n",
      "    5          -     -     -     -     -     -   -              \n",
      "    6          -     -     -     -     -     -   -              \n",
      "    7          -     -     -     -     -     -   -              \n",
      "\n",
      "Set env: CUDA_VISIBLE_DEVICES=[2].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = Namespace(image = f'deepfillv1/examples/places2/sunset_input.png',\n",
    "                 mask = f'deepfillv1/examples/places2/sunset_mask.png',\n",
    "                 flow = f'./flow.png',\n",
    "                 output = f'./output.png',\n",
    "                 output_coarse = f'./output_coarse.png',\n",
    "                 output_fine = f'./output_fine.png',\n",
    "                 output_flow = f'./output_flow.png',\n",
    "                 output_modulated = f'./output_modulated.png',\n",
    "                 checkpoint_dir = f'deepfillv1/model_logs')\n",
    "\n",
    "ng.get_gpus(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention output test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image: (512, 680, 3)\n",
      "Shape of model input: (1, 512, 1360, 3)\n"
     ]
    }
   ],
   "source": [
    "model = InpaintCAModel()\n",
    "image = cv2.imread(args.image)\n",
    "mask = cv2.imread(args.mask)\n",
    "\n",
    "assert image.shape == mask.shape\n",
    "\n",
    "h, w, _ = image.shape\n",
    "grid = 8\n",
    "image = image[:h//grid*grid, :w//grid*grid, :]\n",
    "mask = mask[:h//grid*grid, :w//grid*grid, :]\n",
    "print('Shape of image: {}'.format(image.shape))\n",
    "\n",
    "image = np.expand_dims(image, 0)\n",
    "mask = np.expand_dims(mask, 0)\n",
    "input_image = np.concatenate([image, mask], axis=2)\n",
    "\n",
    "print('Shape of model input: {}'.format(input_image.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called: build_server_graph()\n",
      "Shape of masked images: (1, 512, 680, 3)\n",
      "Shape of masks: (1, 512, 680, 1)\n",
      "Called: build_inpaint_net()\n",
      "Shape of first-stage input: (1, 512, 680, 5)\n",
      "Shape of first-stage output: (1, 512, 680, 3)\n",
      "Shape of second-stage input: (1, 512, 680, 3)\n",
      "Shape of second-stage conv branch output: (1, 128, 170, 128)\n",
      "Shape of contextual attention input: (TensorShape([Dimension(1), Dimension(128), Dimension(170), Dimension(128)]), TensorShape([Dimension(1), Dimension(128), Dimension(170), Dimension(1)]))\n",
      "Called: contextual_attention()\n",
      "Shape of foreground features: (1, 128, 170, 128)\n",
      "Shape of background features: (1, 128, 170, 128)\n",
      "Shape of masks: (1, 128, 170, 1)\n",
      "Shape of background patches: (1, 4, 4, 128, 5440)\n",
      "Shape of resized foreground features: (1, 64, 85, 128)\n",
      "Shape of resized background features: (1, 64, 85, 128)\n",
      "Shape of resized masks: (1, 64, 85, 1)\n",
      "Shape of downscaled background patches (w): (1, 3, 3, 128, 5440)\n",
      "Shape of mask patches: (1, 1, 1, 5440)\n",
      "[1/1]: Shapes fg (1, 64, 85, 128), bg (1, 3, 3, 128, 5440)/(1, 4, 4, 128, 5440)\n",
      "[1/1]: Shape of convolved features (1, 64, 85, 5440)\n",
      "Shape of contextual attention output: (1, 128, 170, 128)\n",
      "Shape of second-stage att branch output: (1, 128, 170, 128)\n",
      "Shape of second-stage output: (1, 512, 680, 3)\n",
      "Shape of coarse output: (1, 512, 680, 3)\n",
      "Shape of fine output: (1, 512, 680, 3)\n",
      "Shape of attention values: (1, 512, 680, 3)\n",
      "Model loaded.\n",
      "Shape of model output: (1, 512, 2720, 3)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "sess_config = tf.ConfigProto()\n",
    "sess_config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "    input_image = tf.constant(input_image, dtype=tf.float32)\n",
    "    output = model.build_server_graph(input_image)\n",
    "    output = (output + 1.) * 127.5\n",
    "    output = tf.reverse(output, [-1])\n",
    "    output = tf.saturate_cast(output, tf.uint8)\n",
    "    # load pretrained model\n",
    "    vars_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    assign_ops = []\n",
    "    for var in vars_list:\n",
    "        vname = var.name\n",
    "        from_name = vname\n",
    "        var_value = tf.contrib.framework.load_variable(args.checkpoint_dir, from_name)\n",
    "        assign_ops.append(tf.assign(var, var_value))\n",
    "    sess.run(assign_ops)\n",
    "    print('Model loaded.')\n",
    "    result = sess.run(output)\n",
    "    print('Shape of model output: {}'.format(result.shape))\n",
    "    \n",
    "    result = np.array(result)\n",
    "    result, coarse, fine, flow = np.split(result, 4, axis=2)\n",
    "    cv2.imwrite(args.output, result[0][:, :, ::-1])\n",
    "    cv2.imwrite(args.output_coarse, coarse[0][:, :, ::-1])\n",
    "    cv2.imwrite(args.output_fine, fine[0][:, :, ::-1])\n",
    "    cv2.imwrite(args.output_flow, flow[0][:, :, ::-1])\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention input test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image: (512, 680, 3)\n",
      "Shape of model input: (1, 512, 1360, 3)\n",
      "Shape of flow input: (1, 512, 680, 3)\n"
     ]
    }
   ],
   "source": [
    "model = InpaintCAModel()\n",
    "image = cv2.imread(args.image)\n",
    "mask = cv2.imread(args.mask)\n",
    "flow = cv2.imread(args.flow)\n",
    "\n",
    "assert image.shape == mask.shape == flow.shape\n",
    "\n",
    "h, w, _ = image.shape\n",
    "grid = 8\n",
    "image = image[:h//grid*grid, :w//grid*grid, :]\n",
    "mask = mask[:h//grid*grid, :w//grid*grid, :]\n",
    "print('Shape of image: {}'.format(image.shape))\n",
    "\n",
    "image = np.expand_dims(image, 0)\n",
    "mask = np.expand_dims(mask, 0)\n",
    "input_image = np.concatenate([image, mask], axis=2)\n",
    "print('Shape of model input: {}'.format(input_image.shape))\n",
    "\n",
    "flow = flow[:h//grid*grid, :w//grid*grid, :]\n",
    "input_flow = np.expand_dims(flow, 0)\n",
    "print('Shape of flow input: {}'.format(input_flow.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called: build_server_graph()\n",
      "Shape of masked images: (1, 512, 680, 3)\n",
      "Shape of masks: (1, 512, 680, 1)\n",
      "Called: build_inpaint_net()\n",
      "Shape of first-stage input: (1, 512, 680, 5)\n",
      "Shape of first-stage output: (1, 512, 680, 3)\n",
      "Shape of second-stage input: (1, 512, 680, 3)\n",
      "Shape of second-stage conv branch output: (1, 128, 170, 128)\n",
      "Shape of contextual attention input: (TensorShape([Dimension(1), Dimension(128), Dimension(170), Dimension(128)]), TensorShape([Dimension(1), Dimension(128), Dimension(170), Dimension(1)]))\n",
      "Called: contextual_attention()\n",
      "Shape of foreground features: (1, 128, 170, 128)\n",
      "Shape of background features: (1, 128, 170, 128)\n",
      "Shape of masks: (1, 128, 170, 1)\n",
      "Shape of background patches: (1, 4, 4, 128, 5440)\n",
      "Shape of resized foreground features: (1, 64, 85, 128)\n",
      "Shape of resized background features: (1, 64, 85, 128)\n",
      "Shape of resized masks: (1, 64, 85, 1)\n",
      "Shape of downscaled background patches (w): (1, 3, 3, 128, 5440)\n",
      "Shape of mask patches: (1, 1, 1, 5440)\n",
      "[1/1]: Shapes fg (1, 64, 85, 128), bg (1, 3, 3, 128, 5440)/(1, 4, 4, 128, 5440)\n",
      "[1/1]: Shape of convolved features (1, 64, 85, 5440)\n",
      "Shape of contextual attention output: (1, 128, 170, 128)\n",
      "Shape of second-stage att branch output: (1, 128, 170, 128)\n",
      "Shape of second-stage output: (1, 512, 680, 3)\n",
      "Shape of coarse output: (1, 512, 680, 3)\n",
      "Shape of fine output: (1, 512, 680, 3)\n",
      "Shape of attention values: (1, 512, 680, 3)\n",
      "Model loaded.\n",
      "Shape of model output: (1, 512, 2720, 3)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "sess_config = tf.ConfigProto()\n",
    "sess_config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "    input_image = tf.constant(input_image, dtype=tf.float32)\n",
    "    output = model.build_server_graph(input_image, input_flow)\n",
    "    output = (output + 1.) * 127.5\n",
    "    output = tf.reverse(output, [-1])\n",
    "    output = tf.saturate_cast(output, tf.uint8)\n",
    "    # load pretrained model\n",
    "    vars_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    assign_ops = []\n",
    "    for var in vars_list:\n",
    "        vname = var.name\n",
    "        from_name = vname\n",
    "        var_value = tf.contrib.framework.load_variable(args.checkpoint_dir, from_name)\n",
    "        assign_ops.append(tf.assign(var, var_value))\n",
    "    sess.run(assign_ops)\n",
    "    print('Model loaded.')\n",
    "    result = sess.run(output)\n",
    "    print('Shape of model output: {}'.format(result.shape))\n",
    "    \n",
    "    result = np.array(result)\n",
    "    result, coarse, fine, flow = np.split(result, 4, axis=2)\n",
    "    cv2.imwrite(args.output_modulated, result[0][:, :, ::-1])\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
